{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check data and build input reading pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/seanyu/.conda/envs/tf18_keras/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n",
      "/home/seanyu/.conda/envs/tf18_keras/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n",
      "/home/seanyu/.conda/envs/tf18_keras/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n",
      "/home/seanyu/.conda/envs/tf18_keras/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n",
      "/home/seanyu/.conda/envs/tf18_keras/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n",
      "/home/seanyu/.conda/envs/tf18_keras/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(full_random=0, gpu_id=None, message=None, split_id=None, with_transfer=1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/seanyu/.conda/envs/tf18_keras/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n",
      "/home/seanyu/.conda/envs/tf18_keras/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "/home/seanyu/.conda/envs/tf18_keras/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/seanyu/.conda/envs/tf18_keras/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/base.py:198: retry (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use the retry module or similar alternatives.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import os\n",
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "import glob\n",
    "import re\n",
    "import random\n",
    "import datetime\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from PIL import Image\n",
    "import cv2\n",
    "\n",
    "import argparse\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--gpu_id', type = str)\n",
    "parser.add_argument('--split_id', type = str, help = 'split_id, from 0 to 7')\n",
    "parser.add_argument('--full_random', default = 0)\n",
    "parser.add_argument('--with_transfer', default = 1)\n",
    "parser.add_argument('--message', type = str, help = \"Recording experiment infomation\")\n",
    "FLAGS = parser.parse_args([])\n",
    "print(FLAGS)\n",
    "\n",
    "FLAGS.gpu_id = \"1\"\n",
    "FLAGS.full_random = 1\n",
    "FLAGS.with_transfer = 0\n",
    "FLAGS.message = \"run it\"\n",
    "\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = str(FLAGS.gpu_id)\n",
    "import tensorflow as tf\n",
    "\n",
    "#from config import Basic_Setup, Hparams, Augmentation_Setup\n",
    "from DataGenerator import Create_Data_Generator\n",
    "\n",
    "sys.path.append(\"/home/seanyu/Common_tools/\")\n",
    "from model import create_model\n",
    "from trainer import model_trainer\n",
    "from CONFIG_MODEL import PRETRAIN_MODEL_SETUP\n",
    "from Recording import Experiment_Recoding\n",
    "from TF_callbacks import (\n",
    "    EarlyStopping, \n",
    "    Model_checkpoint, \n",
    "    ReduceLROnPlateau, \n",
    "    Model_Timer,\n",
    "    Run_collected_functions\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/seanyu/.conda/envs/tf18_keras/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "class Basic_Setup(object):\n",
    "    data_dir = \"/data/seanyu/UPMC_pathology/Pathology_Analysis/data/\" # path to data\n",
    "    label_dir = \"/data/seanyu/UPMC_pathology/Pathology_Analysis/splits/\" # path to label\n",
    "    result_dir = \"/data/seanyu/UPMC_pathology/result/resnet50_RandomSplit_FromScratch\" # path to store results\n",
    "    model_saving_name = \"model\" # model name prefix\n",
    "    do_augmentation = True\n",
    "    \n",
    "    n_threads = 8\n",
    "    dq_size = 50\n",
    "    \n",
    "# ======================= #\n",
    "try:\n",
    "    import imgaug as ia\n",
    "    from imgaug import augmenters as iaa\n",
    "except:\n",
    "    print(\"Import Error, Please make sure you have imgaug\")\n",
    "        \n",
    "try:\n",
    "    sys.path.append(\"Common_tools/\")\n",
    "    from customized_imgaug_func import keypoint_func, img_channelswap\n",
    "except:\n",
    "    print(\"Warning, if you used customized imgaug function\")\n",
    "\n",
    "class Augmentation_Setup(object):  \n",
    "    sometimes = lambda aug: iaa.Sometimes(0.5, aug)\n",
    "    lesstimes = lambda aug: iaa.Sometimes(0.2, aug)\n",
    "    \n",
    "    augmentation = iaa.Sequential([\n",
    "        iaa.Fliplr(0.5, name=\"FlipLR\"),\n",
    "        iaa.Flipud(0.5, name=\"FlipUD\"),\n",
    "        iaa.ContrastNormalization((0.8, 1.2), name = \"Contrast\"),\n",
    "        iaa.Add((-15, 15), per_channel = 0.5),\n",
    "   #     iaa.OneOf([iaa.Multiply((0.8, 1.2), per_channel = 0.5, name = \"Multiply\"),\n",
    "   #                iaa.AddToHueAndSaturation((-15,15),name = \"Hue\"),\n",
    "   #               ]),\n",
    "        sometimes(iaa.GaussianBlur((0, 1.5), name=\"GaussianBlur\")),\n",
    "        iaa.OneOf([iaa.Affine(rotate = 90),\n",
    "                   iaa.Affine(rotate = 180),\n",
    "                   iaa.Affine(rotate = 270)]),\n",
    "        sometimes(iaa.Affine(\n",
    "                    scale = (0.6,1.2),\n",
    "                    #translate_percent = (-0.2, 0.2),\n",
    "                    rotate = (-15, 15),\n",
    "                    mode = 'wrap'\n",
    "                    ))\n",
    "    ])\n",
    "    \n",
    "# ============================== #\n",
    "import tensorflow as tf\n",
    "class Hparams(object):\n",
    "    \"\"\"\n",
    "    Model related setup\n",
    "    \"\"\"\n",
    "    input_size = (256, 256, 3)\n",
    "    batch_size = 32\n",
    "    epochs = 200\n",
    "    n_batch = 300\n",
    "    n_class = 2\n",
    "    class_weight = None # leave None if don't use it\n",
    "    optimizer = 'adam'\n",
    "    pretrain_model = \"resnet_50\"\n",
    "    \n",
    "    learning_rate = 0.00017\n",
    "    reduce_lr_options = {'lr':learning_rate, 'factor':0.5, 'patience':4}\n",
    "    earlystop_options = {'min_delta': 1e-4, 'patience':9}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/data/seanyu/UPMC_pathology/result/resnet50_RandomSplit_FromScratch has already exist, overwrite it\n",
      "Write recording file to /data/seanyu/UPMC_pathology/result/resnet50_RandomSplit_FromScratch/msg.txt\n",
      "copying /home/seanyu/project/UPMC_mitosis/config.py to /data/seanyu/UPMC_pathology/result/resnet50_RandomSplit_FromScratch/config.py\n",
      "copying /home/seanyu/project/UPMC_mitosis/model.py to /data/seanyu/UPMC_pathology/result/resnet50_RandomSplit_FromScratch/model.py\n",
      "(1095, 256, 256, 3)\n",
      "== INITIALIZE PARAMETERS ==\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Training batch:   0%|          | 0/300 [00:00<?, ?batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start time: Tue Oct  2 12:16:46 2018\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch: 300, Training loss/acc: 0.46/0.78: 100%|██████████| 300/300 [01:23<00:00,  3.61batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/200\n",
      "train loss: 0.46340322494506836 | val loss: 0.25835558772087097\n",
      "== Validation loss has an improvement, save model ==\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Training batch:   0%|          | 0/300 [00:00<?, ?batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved in path: /data/seanyu/UPMC_pathology/result/resnet50_RandomSplit_FromScratch/model.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch: 300, Training loss/acc: 0.39/0.83: 100%|██████████| 300/300 [01:11<00:00,  4.17batch/s]\n",
      "Training batch:   0%|          | 0/300 [00:00<?, ?batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2/200\n",
      "train loss: 0.3852893114089966 | val loss: 0.29833465814590454\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch: 300, Training loss/acc: 0.33/0.86: 100%|██████████| 300/300 [01:12<00:00,  4.16batch/s]\n",
      "Training batch:   0%|          | 0/300 [00:00<?, ?batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3/200\n",
      "train loss: 0.3273095190525055 | val loss: 0.41679713129997253\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch: 300, Training loss/acc: 0.30/0.87: 100%|██████████| 300/300 [01:12<00:00,  4.16batch/s]\n",
      "Training batch:   0%|          | 0/300 [00:00<?, ?batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4/200\n",
      "train loss: 0.2999616861343384 | val loss: 0.29430636763572693\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch: 300, Training loss/acc: 0.28/0.89: 100%|██████████| 300/300 [01:13<00:00,  4.10batch/s]\n",
      "Training batch:   0%|          | 0/300 [00:00<?, ?batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5/200\n",
      "train loss: 0.28029051423072815 | val loss: 0.4630087614059448\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch: 92, Training loss/acc: 0.25/0.89:  31%|███       | 92/300 [00:22<00:49,  4.17batch/s]"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    tf.reset_default_graph()\n",
    "    FLAGS.split_id = str(i)\n",
    "    recording_manager = Experiment_Recoding(Basic_Setup.result_dir, message=FLAGS.message)\n",
    "    start_time = time.time()\n",
    "    df = pd.DataFrame()\n",
    "    for root_path, middle_path, target_file in os.walk(Basic_Setup.label_dir):\n",
    "        if len(target_file) == 0:\n",
    "            pass\n",
    "        else:\n",
    "            for sub_file in target_file:\n",
    "                full_name = os.path.join(root_path, sub_file)\n",
    "                tmp_files = pd.read_csv(full_name, header=None)\n",
    "                df = pd.concat((df, tmp_files), axis = 0)\n",
    "    df.columns = [\"pid\",\"cate\"]\n",
    "    df.drop_duplicates(inplace=True)\n",
    "    df['img_path'] = df['pid'].apply(lambda x: os.path.join(Basic_Setup.data_dir, 'image' + str (x) + \".png\"))\n",
    "    training_files, validation_files = train_test_split(df, test_size = 0.1, stratify =df.cate.values.astype('int')) # around 1:10\n",
    "\n",
    "    train_0 = training_files[training_files['cate'] == 0]\n",
    "    train_1 = training_files[training_files['cate'] == 1]\n",
    "    train_0.reset_index(drop=True, inplace=True)\n",
    "    train_1.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    data_gen = Create_Data_Generator(df = [train_0, train_1],\n",
    "                                     image_size=Hparams.input_size,\n",
    "                                     batch_size=Hparams.batch_size, n_classes=Hparams.n_class,\n",
    "                                     nd_inputs_preprocessing_handler=PRETRAIN_MODEL_SETUP.CORRESPONDING_PREPROC[Hparams.pretrain_model],\n",
    "                                     do_augment=Basic_Setup.do_augmentation, aug_params=Augmentation_Setup.augmentation\n",
    "                                    )\n",
    "\n",
    "    data_gen.start_train_threads(jobs=Basic_Setup.n_threads, dq_size=Basic_Setup.dq_size)\n",
    "    train_gen = data_gen.get_data()\n",
    "    x_val, y_val = data_gen.get_evaluate_data(validation_files)\n",
    "    print(x_val.shape)\n",
    "\n",
    "    cb_dict = {\n",
    "        'reduce_lr' : ReduceLROnPlateau(lr=Hparams.reduce_lr_options['lr'], \n",
    "                                        factor=Hparams.reduce_lr_options['factor'], \n",
    "                                        patience=Hparams.reduce_lr_options['patience']),\n",
    "        'earlystop' : EarlyStopping(min_delta = Hparams.earlystop_options['min_delta'], \n",
    "                                    patience= Hparams.earlystop_options['patience']),\n",
    "        'checkpoint' : Model_checkpoint(model_name=Basic_Setup.result_dir + '/' +  Basic_Setup.model_saving_name, \n",
    "                                        save_best_only=True),\n",
    "        'model_timer': Model_Timer(),\n",
    "    }\n",
    "\n",
    "    callback_dict = {\n",
    "        'on_session_begin':[cb_dict['model_timer']], # start of a session\n",
    "        'on_batch_begin':[], # start of a training batch\n",
    "        'on_batch_end':[], # end of a training batch\n",
    "        'on_epoch_begin':[], # start of a epoch\n",
    "        'on_epoch_end':[cb_dict['earlystop'], \n",
    "                        cb_dict['reduce_lr'],\n",
    "                        cb_dict['checkpoint']], # end of a epoch\n",
    "        'on_session_end':[cb_dict['model_timer']] # end of a session\n",
    "        }\n",
    "    callback_manager = Run_collected_functions(callback_dict)\n",
    "\n",
    "    model_ops, metric_history = create_model(Hparams)\n",
    "    trainer = model_trainer(hflags=Hparams, # hyper-parameters\n",
    "                            data_gen=data_gen, # data generator, for get infos\n",
    "                            data_gen_get_data = train_gen, # for yield patches\n",
    "                            model_ops=model_ops, # model graph\n",
    "                            metric_history=metric_history, # metric recording\n",
    "                            callback_manager=callback_manager # runable callbacks\n",
    "                            )\n",
    "    trainer.initalize()\n",
    "    if FLAGS.with_transfer:\n",
    "        pretrain_ckpt = PRETRAIN_MODEL_SETUP.PRETRAIN_DICT[Hparams.pretrain_model]\n",
    "        trainer.restore(model_to_restore=pretrain_ckpt, partial_restore=True)\n",
    "    trainer.do_training(cb_dict=cb_dict, validation_set=(x_val, y_val))\n",
    "\n",
    "    val_pred = trainer.predict(x_= x_val, \n",
    "                               model_to_restore=os.path.join(Basic_Setup.result_dir, Basic_Setup.model_saving_name) + '.ckpt', \n",
    "                               bz=Hparams.batch_size)\n",
    "\n",
    "    val_pred = np.concatenate([item for sublist in val_pred for item in sublist])\n",
    "    acc = np.sum(val_pred.argmax(axis = 1) == y_val.argmax(axis = 1)) / len(y_val)\n",
    "    print(acc)\n",
    "\n",
    "    validation_files['y_pred'] = val_pred[:,1]\n",
    "\n",
    "    if not FLAGS.full_random:\n",
    "        output_df_name = os.path.join(Basic_Setup.result_dir, 'result_sp' + str(FLAGS.split_id) + '.csv')\n",
    "        history_file_name = os.path.join(Basic_Setup.result_dir, 'log_sp' + str(FLAGS.split_id) + '.csv')\n",
    "    else:\n",
    "        output_df_name = os.path.join(Basic_Setup.result_dir, 'result_randomsp' + str(FLAGS.split_id) + '.csv')\n",
    "        history_file_name = os.path.join(Basic_Setup.result_dir, 'log_randomsp' + str(FLAGS.split_id) + '.csv')\n",
    "\n",
    "    validation_files.to_csv(path_or_buf=output_df_name, index=False)\n",
    "\n",
    "    #recording_manager.write_new_msg('Validation Performance Accuracy: %.3f' % (acc) )\n",
    "\n",
    "    history_logs = pd.DataFrame({'train_accuracy': trainer.metric_history['accuracy']['train'],\n",
    "                                 'valid_accuracy': trainer.metric_history['accuracy']['valid']})\n",
    "\n",
    "    history_logs.to_csv(history_file_name, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
